[{"id":0,"href":"/erp/docs/table-of-contents/training/part_1/","title":"1. Getting started with ERPLAB","section":"ERP Training Resources","content":"\nGetting started (or \u0026lsquo;help, how do I make ERPLAB work?\u0026rsquo;)\r#\rIntro\rTo conduct any of the analyses featured in this tutorial, you require three downloads.\nFirstly, MATLAB. MATLAB is a very powerful (see also \u0026lsquo;very expensive\u0026rsquo;) software programme. However, in order to use EEGLAB MATLAB is a pre-requisite. Luckily however they offer some pretty great discounts for students, so it\u0026rsquo;s well worth buying a student copy if that\u0026rsquo;s a possibility. Universities will also often have a MATLAB license that you\u0026rsquo;ll be able to use either on campus, or link to via your personal computer.\nNext, you\u0026rsquo;ll need to download EEGLAB. EEGLAB is super handy open-access software intended for research purposes. EEGLAB is a necessary pre-requisite to ERPLAB, with which we\u0026rsquo;ll conduct most of our pre-processing.\nSteps: Download EEGLAB from the above link. Extract the folder, then open MATLAB. Navigate to \u0026lsquo;set path\u0026rsquo;, which you can find in the Home tab in the Environment section. Select \u0026lsquo;Add Folder\u0026rsquo; and navigate to the EEGLAB folder. You may then wish to select EEGLAB in the \u0026lsquo;MATLAB search path\u0026rsquo; list and move it to the top, with the help of the \u0026lsquo;Move to top\u0026rsquo; button on the left-hand side. For more details, click here\nFinally, you will need to download the ERPLAB toolbox, some more great open-access software that will enable you to conduct pretty much any pre-processing operation you could want. To learn more about ERPLAB, have a read through this article by Lopex-Calderon \u0026amp; Luck.\nSteps: Download ERPLAB from the above link. Extract the folder, Then place the extracted ERPLAB folder into your EEGLAB plugins folder.\n(e.g., C:/Users/Jen/Documents/MATLAB/eeglab2022.0/plugins/)\nN.B I am in no way affiliated with any of the above mentioned companies/products/resources. I gain no financial incentives for providing the links here or if you click on them, but have done so to make things a little easier to find.\n"},{"id":1,"href":"/erp/docs/table-of-contents/","title":"About","section":"Docs","content":"\nThe resources on these pages are predominantly intended for use by undergraduate, postgraduate, and PhD students wishing to learn to use the powerful (yet initially complex) EEGLAB and ERPLAB software for data pre-processing. The website is owned \u0026amp; maintained by Jen Lewendon, postdoctoral researcher at New York University Abu Dhabi.\nWhilst a myriad of resources are available for the novice ERP researcher, the information on this website aims to fill a gap. Firstly, the ERP Training Resources pages aim to provide a full tutorial from instillation of EEG \u0026amp; ERP lab, to UI or script-based pre-processing and analysis. This should support the user with no prior MATLAB or EEGLAB/ERPLAB experience to get started with EEG data pre-processing. The course also offers practice datasets, scripts, and other resources to consolidate learning and allow for hands-on practice of the full pre-processing pipeline without the need for the user to acquire or source their own data. The course is particularly geared to support Mandarin speaking students to familiarise themselves with the often complex terminology and concepts of pre-processing, offering a wealth of translated content.\nSecondly, the External Resources \u0026amp; Publications section of this website provides a repository of key literature to guide the learner through every stage of the ERP research process, from experiment design, data acquisition, pre-processing \u0026amp; analysis, and reporting. I don\u0026rsquo;t claim any ownership of these resources, but find them very useful in my own research.\nThirdly, the Resources for Researchers section features a number of databases and tools that may prove useful for ERP researchers conducting research into auditory ERP responses. This comes from my own work, and is mostly unpublished resources.\nFinally the Teaching Material section combines both my own materials and external content to help design course content at tertiary level for ERP studies.\nPlease do share the website with anyone for whom the resources might be useful. For any enquiries, help, or further information, contact Jen via her personal website here.\n"},{"id":2,"href":"/erp/docs/table-of-contents/external/design/","title":"Designing your study","section":"External Resources \u0026 Publications","content":" Thinking about your design\r#\rDesigning you study is a key element of ERP research design. Multiple considerations can guide your design to ensure that valuable time spent acquiring, pre-processing and analysing data produces valuable (and hopefully publishable) results.\rWhilst I list a number of resources here, one main considerations that most ERP research overlooks one important principle. Termed the Hillyard Principle (see Luck, 2014), by Steve Luck, the basic premise that ERP responses should be elicited by stimuli that are - in principle - physically identical. Many ERP studies violate this principle, mostly because it is usually far easier to create stimuli that do not adhere to it.\nI want to run a study that examines how people respond to words that are related vs unrelated (see this article if you\u0026rsquo;re unsure which component this experiment might study). Below are examples of my stimuli:\nPrime Target\nRelated: dog cat\nUnrelated: apple garage\nHere, my targets (which I\u0026rsquo;ll measure my ERP response to) are physically different. This introduces a potential secondary factor that might influence any results I find. Instead of being able to attribute any differences in my ERPs to related vs unrelated words, it\u0026rsquo;s very possible that a host of features (including the word length, orthography, frequency\u0026hellip; etc. etc.) might actually be the driving force behind any differences I find. Instead I could improve my design as follows:\nPrime Target\nRelated: dog cat\nUnrelated: apple cat\nNow my ERPs are elicited by the word cat in both conditions, meaning that any physical differences between the conditions must arise purely from their relationship to the prime.\nThe Hillyard Priciple is by no means the only thing to consider in your design, but is a key design issue that necessitates further attention than it is typically afforded. Other important literature\r#\rGeneral design\r#\rPicton, T. W., Bentin, S., Berg, P., Donchin, E., Hillyard, S. A., Johnson Jr, R., \u0026hellip; \u0026amp; Taylor, M. J. (2000). Guidelines for using human event‐related potentials to study cognition: Recording standards and publication criteria.\nOptimising your paradigm for a given component\r#\rKappenman, E. S., Farrens, J. L., Zhang, W., Stewart, A. X., \u0026amp; Luck, S. J. (2021). ERP CORE: An open resource for human event-related potential research. NeuroImage, 225, 117465.\nSample size \u0026amp; power\r#\rPolitzer-Ahles (unpublished) ERP power analyzer.\nClayson, P. E., Carbine, K. A., Baldwin, S. A., \u0026amp; Larson, M. J. (2019). Methodological reporting behavior, sample sizes, and statistical power in studies of event‐related potentials: Barriers to reproducibility and replicability. Psychophysiology, 56(11), e13437.\nLarson, M. J., \u0026amp; Carbine, K. A. (2017). Sample size calculations in human electrophysiology (EEG and ERP) studies: A systematic review and recommendations for increased rigor. International Journal of Psychophysiology, 111, 33-41.\nMisuse of null-hypothesis testing for nuisance effects\r#\rSassenhagen, J., \u0026amp; Alday, P. M. (2016). A common misapplication of statistical inference: Nuisance control with null-hypothesis significance tests. Brain and language, 162, 42-45.\n"},{"id":3,"href":"/erp/docs/table-of-contents/training/","title":"ERP Training Resources","section":"About","content":" The following pages are intended to form a complete course for beginners on ERP pre-processing and analysis in MATLAB. The course assumes no prior knowledge of MATLAB or ERPLAB, but does require a basic understanding of EEG/ERP data and pre-processing stages. The steps here follow guidelines for optimal ERP pre-processing steps in the order sppecified by Luck \u0026amp; Kappenman, taking into consideration the important concept of linear and non-linerar operations.\nThe course is split into X parts. Each part is intended to only last a couple of minutes, allowing you to dip in and out as required. The format is (mostly) consistent across each parts as follows:\nA brief description of the pre-processing operations; A video to guide you through conducting the pre-processing operation(s) via the user interface (UI); The code equivalent of the pre-processing steps conducted; A complete script, compiling all pre-processing operations conducted up to (and including) the current page; An example dataset (so that you can have a go); Where relevant, a brief activity or task to consolidate understanding. A written summary of the operation, reflecting gold-standard pre-processing reporting (1); FAQs You are free (in fact, welcome) to use all and any of the resources on these pages. Everything is released under a Creative Commons Attribution 4.0 License which means you can do anything you want with these materials and can change/adapt them in any way you like, as long as you acknowledge that you got it from here. My primary motive for creating these pages was to pass along all the help and guidance I received whilst first learning EEG. Nonetheless, I\u0026rsquo;ve been told to be more career-minded and say that citations would be immensely useful to me if you use my scripts or tutorial for your data analysis, as I am early stage researcher still at the bottom of the academic food chain.\nCite as Lewendon, J. (2023). ERP Training Resources. https://j-lewen.github.io/erp/.\nFootnotes\r#\r(1) See Appendix A for the complete pre-processing paragraph that reflects the exact pipeline followed in this tutorial. You are welcome to copy and paste either this, or the text lines provided for each stage in its exact format into your work (in fact I encourage you to do so to improve ERP reporting standards), so long as you cite these resources in your paper. "},{"id":4,"href":"/erp/docs/table-of-contents/training/part_2/","title":"2. Importing data","section":"ERP Training Resources","content":"\nImporting data (or \u0026lsquo;help, how do I get started?\u0026rsquo;)\r#\rIntro\rNow you\u0026rsquo;ve got ERPLAB up and running, it\u0026rsquo;s time to have a look at some data! But how do you get your data into ERPLAB in the first place? Take a look at the video below for a brief guide to the process of importing your data.\nVideo\rCode EEG = pop_loadcnt('C:\\[DATASET ADDRESS]\\[DATASET NAME].cnt' , 'dataformat', 'auto', 'memmapfile', ''); [ALLEEG EEG CURRENTSET] = pop_newset(ALLEEG, EEG, 0,'setname','[DATASET NAME FOR ERPLAB]','gui','off'); Script Script #1 (download).\nScript #1 (view).\nDataset To run this operation via the user interface, the example data set (used in the above video) can be downloaded here.\nActivity\rHave a go at importing Dataset #1 into EEGLAB using both the user interface and script methods. For the script method, try saving Dataset_1 (e.g., Dataset_1, Data_set2) with a different name, and see if you can import multiple datasets using the Script 1. Finally, save your version of the script (with all the amended folder paths) to your computer so as to ensure you have a working script for subsequent tutorial sections (and your own data analysis!).\nFAQs\rWhy won\u0026#39;t my dataset load?\r↕\rEnsure that you are attempting to import the correct file type. Remember that the instructions here are for .CNT file types, but the steps themselves transferable across a range of file types.\rMy dataset isn\u0026#39;t in any of the listed formats. What should I do?\r↕\rMost data acquisition software offers the ability to convert your file type. For example, I use software that produces a range of files that are incompatible with EEGLAB. However, I am able to save these files in a compatible format within the acquisition software. If you\u0026rsquo;re unsure of how to do this, speak to your EEG technician, researchers around you, or (always a decent resort), Google.\rI tried to run the script but it didn\u0026#39;t work. Help!\r↕\rEnsure you\u0026rsquo;ve read the Scripts: Essential information page, which provides further information on the file and folder structures necessary to run scripts available on this site. Always check that your path to the experiment folder and EEGLAB are correct\r"},{"id":5,"href":"/erp/docs/table-of-contents/external/acquisition/","title":"Acquiring your data","section":"External Resources \u0026 Publications","content":" Acquiring your data\r#\rEEG protocol for research Farrens, J. L., Simmons, A. M., Luck, S. J., \u0026amp; Kappenman, E. S. (2021). Electroencephalogram (EEG) recording protocol for cognitive and affective human neuroscience research. "},{"id":6,"href":"/erp/docs/table-of-contents/teaching/","title":"Teaching Material","section":"About","content":"under construction\n"},{"id":7,"href":"/erp/docs/table-of-contents/training/part_3/","title":"3. Setting channel locations","section":"ERP Training Resources","content":"\nChannel locations\r#\rIntro\rSo you\u0026rsquo;ve got your data open. But the problem is, EEGLAB has no idea which channel goes where. At present, all it knows is your channel IDs (names/numbers) if you\u0026rsquo;re lucky. So your next step is to tell EEGLAB where these channels belong in relation to one another by using a location file. Luckily, the creators of EEGLAB have provided a channel file which features a database of 385 defined channel labels. As long as your data was imported into EEGLAB with these labels (which most systems do) then sourcing your channel locations from this file should typically work well.\nVideo\rCode EEG=pop_chanedit(EEG, 'lookup',[pathtoeeglab '\\\\plugins\\\\[CHANNEL ADDRESS]]']);\rScript\rScript #2 (download).\nScript #2 (view).\nDataset To run this operation via the user interface, the example data set (used in the above video), along with the channel location file can be downloaded here.\nActivity\rHave a go at setting channel locations for your dataset using both the UI and the scripts provided.\nFAQs\rHow do I delete a channel?\r↕\rFirstly, have a think. Why might you want to do this? Perhaps your EEG system has recorded a channel that wasn\u0026rsquo;t intended (i.e., EKG), or you selected a layout during data recording for one participant that includes electrodes not present in your other datasets (making it difficult to grand average across datasets with different numbers of channels at a later stage). Most of the time it is not necessary to manually remove a channel unless there is a discrepancy between the number of channels present across datasets. However, if this is the ase, simply selecting \u0026lsquo;delete channel\u0026rsquo; on the edit channel info menu will not remove the channel from a dataset. Instead, you should go to Edit\u0026gt;Select data\u0026gt;Channel range, and by ticking the \u0026lsquo;remove these\u0026rsquo; checkbox can select any channels you wish to remove.\n\u0026lt;/div\u0026gt;\r"},{"id":8,"href":"/erp/docs/table-of-contents/external/","title":"External Resources \u0026 Publications","section":"About","content":" The use of ERP methodology involves numerous degrees of freedom. To ensure the reliability and replicability of ERP research output, it is therefore crucial that researchers avoid practises (all too common to the field) prone to result in erroneous effects, and ensure that their reporting is complete and replicable. To support researchers to do so, numerous efforts have been made to improve how people use ERP as a methodology, and ensure that reporting of data acquisition, pre-processing, analyses, and results are standardised. Such tools, frameworks, and guidance harbour the potential to substantially increase replicability and reliability in the field, and tackle the ongoing replication crisis.\nThese resources provide us with an invaluable opportunity to substantially improve standards in ERP research and reporting. As the field continues to expand, the implementation of these guidelines and recommendations in ERP study design, data collection, analysis, and reporting is crucial to ensure the reliability and validity of scientific developments. However, despite the longstanding availability of such resources, their use (and adherence to their guidance) is scarce (see, for example. common under-reporting of methodological detail; infrequent use of sample size calculations).\nThe literature cited within these pages serves as a repository for resources that can guide every stage of your ERP research through design, data acquisition, pre-processing, and reporting to ensure that the research you produce make a valuable contribution to the field.\nNB. It goes without saying that the first resource for any ERP researcher in the making is An Introduction to the Event-Related Potential Technique by Steve Luck. If finances are of concern (which, of course, for most students it is) consider the first edition, which contains much of the useful content and can be sourced second-hand for quite a bit less.\n"},{"id":9,"href":"/erp/docs/table-of-contents/external/preprocess/","title":"Pre-processing \u0026 analysis","section":"External Resources \u0026 Publications","content":" Pre-processing \u0026amp; analysis\r#\rPerhaps the second major time investment in any ERP study is the preprocessing that must occur subsequent to data acquisition, before you can even begin to analyse or interpret your data. However, this is where degrees of experimenter freedom increase substantially, resulting in practices that are either intentionally or unintentionally less than ideal. In addition to this, the analyses you choose (and how you choose them) require considerable thought. Below, I have outlined just a few considerations.\nThe problem of multiple implicit comparisons\r#\rOne of the most widely accepted/practiced techniques in ERP data analysis (but importantly, one that is also widely accepted to be flawed) is the selection of time windows and electrodes in order to calculate mean/peak amplitude by looking at where an effect seems to happen. This might seem like an intuitive process, but the implications for the validity of any results that follow from it are considerable. For an excellent paper on See this article by Luck and Gaspelin (2017)\nPreprocessing \u0026amp; publication\r#\rKeil, A., Debener, S., Gratton, G., Junghöfer, M., Kappenman, E. S., Luck, S. J., \u0026hellip; \u0026amp; Yee, C. M. (2014). Committee report: publication guidelines and recommendations for studies using electroencephalography and magnetoencephalography. Psychophysiology, 51(1), 1-21\nFiltering (and what inappropriate filtering can do to your data)\r#\rTanner, D., Morgan‐Short, K., \u0026amp; Luck, S. J. (2015). How inappropriate high‐pass filters can produce artifactual effects and incorrect conclusions in ERP studies of language and cognition. Psychophysiology, 52(8), 997-1009.\n"},{"id":10,"href":"/erp/docs/table-of-contents/training/part_4/","title":"4. Resampling","section":"ERP Training Resources","content":"\nResampling\r#\rIntro\rResampling is a (relatively) simple process that can basically be thought of as deciding how much detail you want to retain in your dataset. Usually, this decision comes down to two things:\nHow important is it for you to measure the precise onset of your effects? Typically, I want to know roughly when my effects begin and end. By reducing the sampling rate I lose a little bit of this information, but not a huge amount. For example, by resampling my data down to 250 Hz I introduce an error of ±2 ms. This is fine for the research I do, but if you\u0026rsquo;re interested in super precise comparisons of the onset of an effect for which 2 ms error would be problematic, you\u0026rsquo;ll want to keep your sampling rate higher. However, this brings us to the next consideration\u0026hellip;\nHow much space do you have on your computer? EEG datasets are typically quite big. Usually you need a reasonable number of them. If you want to run two studies that are both about an hour long, with 40 participants for each and to keep you data at a sampling rate of 1000 Hz, you\u0026rsquo;re going to quickly use up a lot of space on your computer. Beyond this, your pre-processing is going to take longer. For simple things such as re-referencing your sampling rate won\u0026rsquo;t make much difference, but for more complex processes such as artifact correction, the increase in time can be quite significant. If you can afford to lose a little bit of temporal resolution, it\u0026rsquo;s usually worthwhile resampling to a lower rate.\nOnce you\u0026rsquo;ve made the decision as to your preferred sampling rate, the process is a relatively quick and simple one, as outlined in the below video.\nVideo\rCode\rEEG = pop_resample( EEG, 250);\rScript\rScript #3 (download).\nScript #3 (view).\nNote that to run this script you should use Dataset #1 in its original .CNT form, as the script runs from the original continuous files (you need a different function to import .fdt and .set EEGLAB files)\nDataset\rTo run this operation via the user interface, the example data set (used in the above video) can be downloaded here.\nActivity\rHave a go at resampling the dataset provided, both via the user interface and using the available script. Finally, save your version of the script to your computer so as to ensure you have an up-to-date script for subsequent tutorial sections (and your own data analysis!).\nWrite-up EEG data were recorded and digitized at the sampling rate of [ENTER SAMPLING RATE OF ORIGINAL DATASET HERE] Hz using a [ENTER MAKE AND MODEL HERE] amplifier and subsequently downsampled offline to 250Hz.\nFAQ\nCan I resample to a different sampling rate than 250 Hz?\r↕\rFor most experiments, a sampling rate of somewhere between 200 - 1000 Hz is ok. Lower sampling rates will mean that your data files are smaller, and processes steps such as ICA run faster. Higher sampling rates make your datasets larger, but give you more fine-grain detail, which can be particularly useful if you want to explore the latency of a response.\r"},{"id":11,"href":"/erp/docs/table-of-contents/external/writing/","title":"Writing your paper","section":"External Resources \u0026 Publications","content":" Writing your paper\r#\rReporting\r#\rKeil, A., Debener, S., Gratton, G., Junghöfer, M., Kappenman, E. S., Luck, S. J., \u0026hellip; \u0026amp; Yee, C. M. (2014). Committee report: publication guidelines and recommendations for studies using electroencephalography and magnetoencephalography. Psychophysiology, 51(1), 1-21\nPicton, T. W., Bentin, S., Berg, P., Donchin, E., Hillyard, S. A., Johnson Jr, R., \u0026hellip; \u0026amp; Taylor, M. J. (2000). Guidelines for using human event‐related potentials to study cognition: Recording standards and publication criteria\n"},{"id":12,"href":"/erp/docs/table-of-contents/training/part_5/","title":"5. High-pass filtering","section":"ERP Training Resources","content":"\nHigh-pass filtering\r#\rIntro\nFiltering is a complex process, and requires some thought and consideration. It\u0026rsquo;s important that you read around on some literature on filtering to understand what you are doing to your data, and the consequences of inappropriate filtering (particularly see this paper for a fantastic overview of the issue). High-pass filtering - the type of filter that allows the higher amplitudes to pass through but cuts out the lower amplitudes - is a really important early step in your pre-processing pipeline. But high-pass filtering can also cause substantial distortions of your data. So why do it?\nAs you record EEG from a participant, you will often notice slow drifts in voltage that occur over time. These can be quite dramatic (as in the picture above) or occur very steadily. These drifts are caused by changes in the conductivity of the skin, which are fairly unavoidable - apart from ensuring your participants don\u0026rsquo;t sweat excessively (see \u0026lsquo;What if I have really sweaty participants?\u0026rsquo; below). High pass filtering predominantly serves to reduce these slow drifts, although will not eliminate them completely.\nAs with all the other pre-processing steps, you will see plenty of examples in the literature of high-pass filtering done badly. But it is also not to be naive of the fact that all filters will distort your data. Because of this, it\u0026rsquo;s crucial to select your filter carefully. If you take nothing else away from this tutorial, remember that:\nHigh-pass filtering should always be done on continuous data to avoid edge artifacts (1) In most instances, a high-pass filter with a 0.1Hz cut-off (half-amplitude) or lower should be used; Choosing your roll-off slope (2) is equally as important and selecting the right cut-off; Filtering is almost always underreported. Don\u0026rsquo;t be an underreporter. Video\rCode\rEEG = pop_basicfilter( EEG, [1:NUMBER OF ELECTRODES] , 'Boundary', 'boundary', 'Cutoff', 0.1, 'Design', 'butter', 'Filter', 'highpass', 'Order', 2 ); Script\rScript #4 (download).\nScript #4 (view).\nNote that to run this script you should use Dataset #1 in its original .CNT form, as the script runs from the original continuous files (you need a different function to import .fdt and .set EEGLAB files)\nDataset\rTo run this operation via the user interface, the example data set (used in the above video) can be downloaded here\nActivity\rHave a go at applying the filters both via the UI and with the provided script on Dataset_1. Play around with different filter settings and have a look at what different roll-off and cut-off thresholds do to the data. Finally, save your version of the script to your computer so as to ensure you have an up-to-date script for subsequent tutorial sections (and your own data analysis!).\nWrite-up Continuous data was high-pass filtered offline, using an IIR Butterworth filter (2nd order) with a half-amplitude cut-off frequency of 0.1Hz and a and 12 dB/octave roll-off.\nFAQ\rI\u0026#39;ve heard the terms \u0026#39;online filter\u0026#39; and \u0026#39;offline filter\u0026#39;. What do these mean?\r↕\rOffline filters refer to pre-processing steps taken subsequent to data acquisition. Online filters are those applied during EEG recording (i.e., by your EEG data acquisition software.).\rCan I use a different half-amplitude cut-off or slope?\r↕\rYes, but be careful with what you\u0026rsquo;re doing, and be sure to read the recommended resources in \u0026ldquo;What else should I know about filtering\u0026rdquo; below. For a High-pass filter, you can use anything from 0.01 to 0.1 Hz (half-amplitude). A higher cut-off is better for datasets with more artifacts, whilst if your data is pretty clean you might want to sway closer to 0.05 or 0.01 Hz.\rWhat else should I know about filtering?\r↕\rFiltering is a seemingly simple, but actually incredibly complex element of the pre-processing pipeline. It is important that you have a good understanding of what you\u0026rsquo;re doing to your data, and how you can distort it by using the wrong setting. If you are new to filtering, I strongly recommend that you read Steve Luck\u0026rsquo;s chapter Filtering and Fourier Analysis in his 2014 book An Introduction to the Event-Related Potential Technique.\rWhat if I have really sweaty participants?\r↕\rUltimately, the quality of the data you collect will dictate the quality of the data in your study. What I mean by this is, although all of the processes outlined in this tutorial can improve things somewhat, if you start with really cruddy data, you\u0026rsquo;ll still finish with really cruddy data. So, what can you do to prevent tonnes of slow drift in your data? First, you want to make sure that your participants are kept as cool as possible. If you have air conditioning in your EEG lab, turn this on before participants arrive and (if possible) between study blocks. Make sure that before you start the capping process your participants take off coats and jumpers (particularly hoodies or items of clothing that will be difficult to take off once they have an EEG cap on their head, even if the do get too hot). Offer your participants water to drink during the session. Finally - and often overlooked - ensure your participants are as relaxed as they can be. Never underestimate the importance of relaxed participants for good data quality. Participants who are tense, stressed, worried about the task, or uncomfortable with the process will sweat more, have tenser shoulder and neck muscles, and overall perform worse on any task you set them. It is completely within you control to make sure participants understand what\u0026rsquo;s happening, don\u0026rsquo;t feel immense pressure to perform perfectly, and feel comfortable to ask questions. This is something I\u0026rsquo;ve so often seen stressed undergrad and MA students overlook, fixated on getting through the sheer quantity of data acquisition they have to complete. But trust me, you (and your participants) will thank yourself for taking the time to keep them happy.\rFootnotes\r#\r(1) High-pass filters use involve computations that require a couple of seconds of data to operate correctly. If you apply the filter to your epoched data, you stand a chance of introducing artifacts at the beginning and end of your epoch\r#\r(2) When you set a cut-off frequency (e.g., 0.1Hz), the power at this frequency does not immediately drop to 0. Instead, there is a gradual reduction of power over a range of frequencies as specified by the roll-off. Changing this roll-off changes show sharply the filter cuts off power at these frequencies, and higher roll-offs (i.e., 24db/oct, 48db/oct) are sharper in terms of the power cut-off than lower roll-offs (e.g., 12dB/oct). Very sharp roll-offs can cause artifacts, so if you're unsure of what you're doing here, it's safest to 1. go through the recommended reading above, and 2. stick to a softer roll-off (e.g., 12dB/oct). "},{"id":13,"href":"/erp/docs/table-of-contents/training/part_6/","title":"6. Re-referencing","section":"ERP Training Resources","content":"\nRe-referencing\r#\rIntro\nRe-referencing is one of the simplest pre-processing operations to carry out. The complexities surrounding referencing have far less to do with the operation itself, but instead with your choice of reference itself.\nFor those new to ERPs, the concept that the the waveform we study actually reflect the difference in electrical potentials between one or multiple electrode site(s) and the reference site can be difficult to comprehend (1). The electrodes we use to record the neural activity that we\u0026rsquo;re interested in are typically termed the \u0026lsquo;active electrodes\u0026rsquo;. The electrical potential is recorded from active electrodes relative to a theoretically discrete site - the reference, which in an ideal world would be an electrically neutral point. However, in reality, and electrically neutral site is hard to come by, as is explained in quite a bit more detail in Luck, 2005 than I can possibly strive to achieve here. In short, the electrode must be on the body of the person we want to record from, and there is no site on the body that is neutral.\nIn the absence of an electrically neutral site, researchers have resorted to all manner of electrode locations. However, it is important here to make the distinction between online and offline references. When you record EEG data within your lab, you will most likely do so using whichever reference site the lab tends to use. This might be the left mastoid (the bony protrusion behind the ear), Cz, the tip of the nose, an electrode simply labelled \u0026lsquo;CMS\u0026rsquo; (common Mode Sense). It is unlikely that you will make any changes to a pre-established lab set-up, and thus will use whatever online reference is at your disposal during data acquisition.\nHowever, you\u0026rsquo;re not stuck with this reference. Having collected the data you can now amend your reference site(s) to any or all of the channels you\u0026rsquo;ve recorded from. Across ERP literature, an enormous amount of variation exists in chosen reference sites, resulting in reproducibility and comparability issues. Because of this, it\u0026rsquo;s important to carefully consider the reference site that best serves your research (see FAQ: How do I choose my offline reference?).\nPerhaps the two most commonly adopted schemes are the mastoid reference, and average reference. There are various arguments for/against either - to briefly sum, there is no unanimous consensus on the best reference scheme to use, although most ERP researchers agree that it would be better for the field at large if we didn\u0026rsquo;t all use different ones. For this reason, for the purposes of this tutorial we will use the average mastoid (2).\nVideo\rCode\rEEG = pop_reref( EEG, [REFERENCE CHANNEL/CHANNELS] );\rScript\rScript #5 (download).\nScript #5 (view).\nDataset\rTo run this operation via the user interface, the example data set (used in the above video) can be downloaded here\nActivity\rSearch for literature on your topic and check which reference site is most typically used. Have a go at re-referencing to the average mastoid and global average using the UI. Then try running this via the script/ Finally, save your version of the script to your computer so as to ensure you have an up-to-date script for subsequent tutorial sections (and your own data analysis!).\nAs a sidenote, after Step 12 - Averaging, or Step 14 - Grand Averaging, you may wish to play around with your chosen reference to see what this does to your waveforms.\nWrite-up Data were recording with the left mastoid as the online reference, and subsequently re-referenced offline for analysis to the average of the left and right mastoid electrodes. (3)\nFAQ How do I choose my offline reference?\r↕\rChoosing your reference is not always a clearcut decision. Unlike certain pre-processing operations, for which there is often a clear answer (e.g., \u0026lsquo;don\u0026rsquo;t choose a high-pass filter cut-off above 0.1Hz unless you have a very good reason to\u0026rsquo;), choosing the right reference for you often depends on the study you\u0026rsquo;re running. A couple of key factors to take into consideration are: (1) What has the field done before? You typically want to ensure your findings are comparable to similar papers/previous literature, so you might be inclined to choose the same reference as the vast majority of papers you cite; (2) Where is the effect of interest? If you are interested in an effect that occurs predominantly over central electrodes, you probably don\u0026rsquo;t want a central-only reference site (as this will most likely subtract a lot of the signal you\u0026rsquo;re interested in from the data); and (3) How many electrodes have you got to play with? Typically, using the global average as your reference works better if you have greater coverage of the scalp (e.g., a 128 electrode cap, as opposed to an 8 electrode cap).\rDoes it matter if I choose to change my reference later?\r↕\rNo. Referencing is a linear operation (see this page for a very helpful overview of linear and non-linear pre-processing steps). What this effectively means for your is that it is fairly unproblematic to re-reference at any stage, and as many times as you like.\rFootnotes\r#\r(1) See this summary for a superb introduction to the concept if the second paragraph above has you perplexed.\n(2) One of the big drawbacks of the average reference is that you might mistakenly be lead to think that effects reported by studies using the average reference are comparable. This is not always the case. A study that has recorded data from 32 electrode sites will have a very different average reference to a study that recorded with 128 electrodes, or with a different electrode layout.\n(3) If an average reference is used, it is important to make it clear to the reader which electrodes were included in any such average.\n"},{"id":14,"href":"/erp/docs/table-of-contents/training/part_7/","title":"7. Artifact correction (ICA)","section":"ERP Training Resources","content":"\nDealing with artifacts - ICA (or \u0026lsquo;help, my data looks a mess\u0026rsquo;)\r#\rIntro\rPreamble: The most important thing to mention here is that there is no substitute for good quality data. You must ensure that the data you collect is the best it possible can be, because there is nothing you can do during pre-processing that can compensate for bad data. That said, even the best data will almost always have periods of noise. Why? because participants are only human. Give them a break in a testing session and they will almost inevitably move more than you thought humanly possible in the space of 30 seconds.\nMost of this noise is impossible to deal with. Muscle movements, eye movements coughs and yawns are simply magnitudes larger than the brainwaves we\u0026rsquo;re interested in measuring. Most of this data will therefore have to be rejected (see 11. Artifact rejection for more info). But this will (by definition) reduce the number of trials you have per condition, and trials = power, and with great power comes great papers, or so the old saying goes.\nEnter Independent Component Analysis (ICA), a means \u0026lsquo;remove\u0026rsquo; the blinks from your data. However, there are some down sides (see FAQ), so don\u0026rsquo;t be fooled into thinking that ICA is a magic cure with no repercussions. Nonetheless, done properly, ICA can offer you a means through which blinks can be deal with without losing swathes of your precious data.\nWhat does ICA do? In overly simplified terms, ICA involves separating out the different sources (e.g., muscle movements, eye blinks, other eye movements, brain activity) that contribute towards your EEG data. Separating the signal into these functionally distinct sources enables us to subtract unwanted sources out (e.g., blinks), leaving beautifully clean EEG data. However, ICA was never originally intended for EEG data, and EEG data violates assumptions of ICA (most researchers know this but use it anyway as an imperfect fix), so it is important to have a solid understanding of the pitfalls of ICA before you subject your data to this technique, so I thoroughly recommend reading around before you charge on in. Because ICA changes our data in ways that aren\u0026rsquo;t always easy to determine, you should always use it for blinks with a degree of caution, sideways eye movements with even more caution, and not much else (unless you\u0026rsquo;re very confident in what you\u0026rsquo;re doing).\nStage 1: Preparing your data for ICA artifact correction\nSo you\u0026rsquo;ve decided to go ahead with ICA. The first thing you need to do is clean you data. This is because (assuming you\u0026rsquo;re using your whole dataset for ICA training) you need to help the algorithm to successfully identify blinks as opposed to other sources of noise. Importantly, the number of independent components is (by necessity) always equal to the number of channels in your dataset. Because of this, you don\u0026rsquo;t want a 20 second coughing fit that your participant had half way through the session to \u0026rsquo;take up\u0026rsquo; 15-odd components.\nIMPORTANT: do you care about response accuracy?\r↕\rDepending on your paradigm you may want to exclude incorrect response trials. If you\u0026rsquo;re lucky, you\u0026rsquo;ve programmed this such that your EEG data contains response triggers. If not, you may need to import accuracy from the experimental output file. In this instance, either import this information prior to data cleaning, or be careful not to cut trails during data cleaning, as this will result in a mismatch between your experimental file and EEG data.\rVideo\rStep 2: Running ICA\nNow that you\u0026rsquo;ve cleaned the data, you\u0026rsquo;re ready to run ICA. The video below will guide you through how to select and remove eye-movement components, but depending on your actual dataset the output from this can be confusing. This is probably one of the ERP pre-processing stages that demands the most experience, but there are a number of resources to help you become familiarised with the process of identifying occular activity, such as this incredibly helpful UCSD Tutorial.\nVideo\rComing soon\r#\rStep 3: Selecting ICA components\nThe video below will guide you through how to select and remove eye-movement components, but depending on your actual dataset the output from this can be confusing. This is probably one of the ERP pre-processing stages that demands the most experience, but there are a number of resources to help you become familiarised with the process of identifying occular activity, such as this incredibly helpful UCSD Tutorial.\nVideo\rComing soon\r#\rStep 4: When the ICA doesn\u0026rsquo;t work\nSometimes your ICA decomposition produces something unpleasant to the eyes\nVideo\rComing soon\r#\rScript\rScript #5 (download).\nScript #5 (view).\nIt\u0026rsquo;s important to note that data cleaning can only be achieved via manual selection of noisy data. This means that from this stage onwards we cannot continue using Dataset #1 as in previous scripts.\nDataset\rTo clean your data and run ICA via the user interface, the example data set (used in the above video) can be downloaded here\nWrite-up Ocular correction was conducted using Independent Component Analysis (ICA) following visual inspection and cleaning of the data. ICA used the RUNICA algorithm with EOG electrodes excluded, and resulted in an average of [ENTER AVERAGE] components removed per participant [ENTER RANGE].\nActivity\rHave a go at cleaning the dataset provided in ERPLAB. Then, visit the UCSD ICLabel Tutorial, where you can practice labelling the source of components based on studying their EEGLAB output.\nWrite-up EEG data were recorded and digitized at the sampling rate of XXX Hz using a XXX amplifier and subsequently downsampled offline to XXX Hz.\nFAQ\rQuestion 1\r↕\rAnswer 1.\rQuestion 2\r↕\rAnswer 2.\r"},{"id":15,"href":"/erp/docs/table-of-contents/training/part_8/","title":"8. Interpolation","section":"ERP Training Resources","content":"\nInterpolation (or \u0026lsquo;oops the eye electrode fell off my participant\u0026rsquo;s face\u0026rsquo;)\r#\rIntro\nWhilst taking a look at your data, you may notice that certain electrodes that seem particularly noisy (i.e., fuzzy or making unpredictable leaps and falls). If this is the case, you may wish to interpolate them. Interpolation - put simply - involves recreating one electrode from its surrounding electrodes. As a function in EEGLAB, interpolation is fairly simple to carry out.\nIt\u0026rsquo;s easy to see why interpolation is useful. However, let\u0026rsquo;s exemplify the pitfalls of interpolation through an example:\nImagine you are a researcher interested in measuring the N400 response to a target. You collect data from a participant, and note that whilst the data overall is of very good quality, a number of electrodes have broken or lost connection during the testing session. These electrodes are Cz, C2, CP1, CPz, and CP2.\nIn theory you could interpolate these electrodes. But you need to think critically about the dataset this would leave you with. There are two problems here. 1. The electrodes that require interpolation are clustered together in a group. This means that rather than rebuilding the signal from immediately surrounding electrodes, any signal that forms part of the reconstructed electrode will need to come from outside of this bad electrode cluster. The more bad electrodes you have in close proximity, the less accurately interpolation can rebuild something close to the true signal that would have been recorded at the electrode site. Problem 2 pertains to your experiment design. If my intention is to measure the N400, the electrodes in my bad cluster are critical. The N400 - as a typically centroparietal component, is typically recorded from central, centroparietal and (sometimes) frontal or parietal sites. If the vast majority of these electrodes bad (i.e., requiring interpolation), you need to consider whether it\u0026rsquo;s reasonable to retain a dataset in your grand average for which 5 integral electrodes have been interpolated. The long and short of it is, use interpolation to your advantage, but make sure the way you use it is reasonable.\nVideo\rCOMING SOON\nCode\rEEG = pop_interp(EEG, [SPECIFY ELECTRODE NUMBER HERE], 'spherical');\rScript\rScript #5 (download).\nScript #5 (view).\nNote that to run this script you should use Dataset #7. From this step onwards we will use Dataset #7 which has been pre-cleaned (ready for ICA), as opposed to Dataset #1.\nDatasets\rTo run this operation via the user interface, the example dataset (used in the above video) can be downloaded here, along with an additional dataset for scripting practice.\nActivity\rHave a go at interpolating electrodes manually via the UI, and via the attached scripts. Note that the scripts provided here enables you to run through interpolation for all participants whilst specifying \u0026lsquo;bad\u0026rsquo; electrodes at the beginning of the code. This is because most participants will need different electrodes interpolating, so specifying a given electrode won\u0026rsquo;t work if you want to use the script to pre-process en-mass. To practise with the script, enter the electrode that you want to interpolate for a given participant in the curly brackets that follows that participant\u0026rsquo;s ID, e.g.:\nbadchans.participant_01 = {'Cz'}; Then note how this creates a \u0026lsquo;badchans\u0026rsquo; list for each participant. See if you can get the script running for the two provided datasets, interpolating whichever electrodes you like.\nWrite-up COMING SOON\nFAQ\rWhat if I am that N400 researcher?\r↕\rSo you\u0026rsquo;ve got a dataset with a number of bad electrodes that are critical to your analysis. First off, silly you. No but seriously - try to fix these things during data acquisition because a 1-2h testing session end up completely wasted if you decide you\u0026rsquo;re done with fixing impedance problems prematurely. But let\u0026rsquo;s assume this is the situation you\u0026rsquo;re in (because we\u0026rsquo;ve all been silly at some point). The above scenario is fairly extreme, and in reality your options are fairly limited for retaining the dataset. But if we assume that you\u0026rsquo;ve not got 5 crucial electrodes to interpolate, but instead perhaps 3, I would suggest considering a few factors to help inform your decisions. Firstly, how many surrounding electrodes remain, and how close are they in proximity to the original electrode? This will depend on how many electrodes you originally recorded from. For example, trying to interpolate 3 electrodes when you have a 12 electrode cap, vs a 128 is a very different scenario. Secondly, did any of the electrodes lose connection for a brief period, and then regain connection again? If so (see FAQ2) it might be worthwhile leaving the electrode alone.\rWhat if an electrode only goes bad for a short period?\r↕\rThere\u0026rsquo;s no clear-cut answer to this one. Do you want to lose an experimental block because an electrode has gone awry, or do you want to lose the true signal from that electrode for the entire dataset. Two questions can factor into this though, firstly - how integral the electrode is to the component you\u0026rsquo;re analysing, and secondly - how many trials per condition you can afford to lose? Generally, if the answer two the first question is \u0026lsquo;very\u0026rsquo;, I personally would be inclined not to interpolate the electrode in the first instance, and see whether I\u0026rsquo;m left with a reasonable number of trials if I do so. If, however, the trial count drops below that which would enable you to retain the dataset in your analysis, then interpolation is probably the right route.\r"},{"id":16,"href":"/erp/docs/table-of-contents/training/part_9/","title":"9. Low-pass filter (optional)","section":"ERP Training Resources","content":"\nLow-pass filter (optional)\r#\rIntro\nComing soon\r#\rVideo\rComing soon\r#\rCode\rN/A\rScript\rScript #X (download).\nScript #X (view).\nDataset\rTo run this operation via the user interface, the example data set (used in the above video) can be downloaded here\nActivity\rFinally, save your version of the script to your computer so as to ensure you have an up-to-date script for subsequent tutorial sections (and your own data analysis!).\nWrite-up COMING SOON\nFAQ\rQ1\r↕\rA1.\rQ2\r↕\rA2.\r"},{"id":17,"href":"/erp/docs/contact/","title":"Contact","section":"Docs","content":"\rContact \u0026amp; info\r#\rEmail: jennifer.lewendon@polyu.edu.hk\nFor any queries about resource content, webpages or any other questions. Personal Website: https://j-lewen.github.io/\nPersonal website with CV, publications \u0026amp; research interests. Google Scholar: https://scholar.google.com/citations?user=oMuNHZoAAAAJ\u0026hl=en\u0026oi=ao\n"},{"id":18,"href":"/erp/docs/table-of-contents/training/part_10/","title":"10. Epoch your data","section":"ERP Training Resources","content":"\nEpoch (or \u0026lsquo;segment\u0026rsquo;, or \u0026lsquo;cut up\u0026rsquo; your data)\r#\rIntro\nEpoching effectively cuts up your data into segments based on a given trigger, or set of triggers. So far, we\u0026rsquo;ve worked exclusively with continuous data. What does this mean? This means that your whole EEG recording is in one long \u0026lsquo;continuous\u0026rsquo; file\u0026hellip;\nSee?\nVideo\rComing soon\r#\rCode\rN/A\rScript\rScript #7 (download).\nScript #7 (view).\nDataset\rTo run this operation via the user interface, the example data set (used in the above video) can be downloaded here\nActivity\rFinally, save your version of the script to your computer so as to ensure you have an up-to-date script for subsequent tutorial sections (and your own data analysis!).\nWrite-up COMING SOON\nFAQ\rHow do I choose my epoch length?\r↕\rA1.\rQ2\r↕\rA2.\r"},{"id":19,"href":"/erp/docs/table-of-contents/training/part_11/","title":"11. Artifact rejection","section":"ERP Training Resources","content":"\nArtifact rejection\r#\rIntro\nComing soon\r#\rVideo\rComing soon\r#\rCode\rComing soon\rScript\rScript #8 (download).\nScript #8 (view).\nDataset\rTo run this operation via the user interface, the example data set (used in the above video) can be downloaded here\nWrite-up COMING SOON\nActivity\rFinally, save your version of the script to your computer so as to ensure you have an up-to-date script for subsequent tutorial sections (and your own data analysis!).\nFAQ\rQ1\r↕\rA1.\rQ2\r↕\rA2.\r"},{"id":20,"href":"/erp/docs/table-of-contents/training/part_12/","title":"12. Averaging","section":"ERP Training Resources","content":"\nAveraging\r#\rIntro\nComing soon\r#\rVideo\rComing soon\r#\rCode\rN/A\rScript\rScript #9 (download).\nScript #9 (view).\nDataset\rTo run this operation via the user interface, the example data set (used in the above video) can be downloaded here\nActivity\rFinally, save your version of the script to your computer so as to ensure you have an up-to-date script for subsequent tutorial sections (and your own data analysis!).\nWrite-up COMING SOON\nFAQ\rWhat is the difference between averaging and grand averaging?\r↕\rA1.\rQ2\r↕\rA2.\r"},{"id":21,"href":"/erp/docs/table-of-contents/training/part_13/","title":"13. Create difference waves (optional)","section":"ERP Training Resources","content":"\nCreate difference waves (optional)\r#\rIntro\nComing soon\r#\rVideo\rComing soon\r#\rCode\rN/A\rScript\rScript #10 (download).\nScript #10 (view).\nDataset\rTo run this operation via the user interface, the example data set (used in the above video) can be downloaded here\nActivity\rFinally, save your version of the script to your computer so as to ensure you have an up-to-date script for subsequent tutorial sections (and your own data analysis!).\nWrite-up COMING SOON\nFAQ\rQ1\r↕\rA1.\rQ2\r↕\rA2.\r"},{"id":22,"href":"/erp/docs/table-of-contents/training/part_14/","title":"14. Grand averaging","section":"ERP Training Resources","content":"\nGrand averaging\r#\rIntro\nComing soon\r#\rVideo\rComing soon\r#\rCode\rN/A\rScript\rScript #11 (download).\nScript #11 (view).\nDataset\rTo run this operation via the user interface, the example data set (used in the above video) can be downloaded here\nActivity\rFinally, save your version of the script to your computer so as to ensure you have an up-to-date script for subsequent tutorial sections (and your own data analysis!).\nWrite-up COMING SOON\nFAQ\rQ1\r↕\rA1.\rQ2\r↕\rA2.\r"},{"id":23,"href":"/erp/docs/table-of-contents/training/part_15/","title":"15. What next?","section":"ERP Training Resources","content":"\nWhat next?\r#\rIntro\nComing soon\r#\rVideo\rCode\rN/A\rScript\rN/A\rDataset\rActivity\rWrite-up COMING SOON\nFAQ\rQ1\r↕\rA1.\rQ2\r↕\rA2.\r"},{"id":24,"href":"/erp/docs/table-of-contents/training/scripts/","title":"Scripts: Essential information","section":"ERP Training Resources","content":"\rHow to use the scripts provided within these pages\r#\rEvery script looks a little different to the \u0026lsquo;code\u0026rsquo; line on the website. The reson for this is that the code is intended to enable you to identify what the pre-processing operation looks like in code form and to create your own scripts, whilst the scripts provided are intended to make your pre-processing more efficient, allowing you to run each pre-processing operation on all your datasets at once. This requires a little more info (i.e., where EEGLAB is stored on your computer, etc.), and therefore means that a couple of steps are necessary before you can use the scripts for your own data.\nPrepare your data files\r#\rAll the scripts features on this website require that your data is stored as follows.\nFirst, you must create a folder specifically for your experiment.\nThen you must create individual folders for each participant, with the name of the datasets matching the name of the folder exactly.\nFinally, you must ensure that all data from that participant is stored within this folder. This includes .CNT files, .fdt and .set files, and behavioural data.\nNext, you can start to tell you script where the necessary folders are on your computer in order to run. At the beginning of each script, the following will almost always be necessary:\nExample script (download).\nExample script (view).\nEnsure that for every script you run from this site, you enter the correct information as outlined in the example scripts above.\nN.B. The scripts on this website could not have been produced without the valuable guidance and help that I received from Dr. Politzer-Ahles (Kansas University) throughout my first postdoc.\n"},{"id":25,"href":"/erp/docs/table-of-contents/training/part_16/","title":"Appendix A","section":"ERP Training Resources","content":"\nPre-processing reporting template\nA generic template for reporting ERP pre-processing steps. Note that this template will not suit everyone, and adaptation/amendments/deletions may be required\n"}]